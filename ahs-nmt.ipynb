{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "857f7a8e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-16T21:45:49.307444Z",
     "iopub.status.busy": "2023-04-16T21:45:49.306500Z",
     "iopub.status.idle": "2023-04-16T21:45:49.318272Z",
     "shell.execute_reply": "2023-04-16T21:45:49.317410Z"
    },
    "papermill": {
     "duration": 0.022709,
     "end_time": "2023-04-16T21:45:49.320466",
     "exception": false,
     "start_time": "2023-04-16T21:45:49.297757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb43050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:45:49.335415Z",
     "iopub.status.busy": "2023-04-16T21:45:49.333914Z",
     "iopub.status.idle": "2023-04-16T21:46:22.309198Z",
     "shell.execute_reply": "2023-04-16T21:46:22.307733Z"
    },
    "papermill": {
     "duration": 32.985249,
     "end_time": "2023-04-16T21:46:22.312176",
     "exception": false,
     "start_time": "2023-04-16T21:45:49.326927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\r\n",
      "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\r\n",
      "Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.7/site-packages (from gdown) (2.28.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from gdown) (3.9.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gdown) (1.16.0)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from gdown) (4.11.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from gdown) (4.64.1)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.26.14)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (2022.12.7)\r\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.7/site-packages (from requests[socks]->gdown) (1.7.1)\r\n",
      "Installing collected packages: gdown\r\n",
      "Successfully installed gdown-4.7.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m/opt/conda/lib/python3.7/site-packages/gdown/cli.py:130: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\r\n",
      "  category=FutureWarning,\r\n",
      "Downloading...\r\n",
      "From (uriginal): https://drive.google.com/uc?id=1ejVLlApcdG77h-zIL7NPbnaiPDsUooLV\r\n",
      "From (redirected): https://drive.google.com/uc?id=1ejVLlApcdG77h-zIL7NPbnaiPDsUooLV&confirm=t&uuid=e5f4393f-bf47-46d7-b53b-e1d90fdb56b2\r\n",
      "To: /kaggle/working/AHS_clear.csv\r\n",
      "100%|████████████████████████████████████████| 186M/186M [00:02<00:00, 88.7MB/s]\r\n",
      "/opt/conda/lib/python3.7/site-packages/gdown/cli.py:130: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\r\n",
      "  category=FutureWarning,\r\n",
      "Downloading...\r\n",
      "From (uriginal): https://drive.google.com/uc?id=1im9nJFcwz--jODqsKYD5qNqbr7st0UhC\r\n",
      "From (redirected): https://drive.google.com/uc?id=1im9nJFcwz--jODqsKYD5qNqbr7st0UhC&confirm=t&uuid=50b56f26-a8b7-4e34-8f80-7972977f1b80\r\n",
      "To: /kaggle/working/AHS_pandas.txt\r\n",
      "100%|█████████████████████████████████████████| 369M/369M [00:02<00:00, 177MB/s]\r\n",
      "/opt/conda/lib/python3.7/site-packages/gdown/cli.py:130: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\r\n",
      "  category=FutureWarning,\r\n",
      "Downloading...\r\n",
      "From (uriginal): https://drive.google.com/uc?id=122xV9jV2FCb9-cNiQPHG7dLoCkk2IsfF\r\n",
      "From (redirected): https://drive.google.com/uc?id=122xV9jV2FCb9-cNiQPHG7dLoCkk2IsfF&confirm=t&uuid=d733fb8e-e7c6-449b-bf8e-b8309bd6e9d3\r\n",
      "To: /kaggle/working/AHS.csv\r\n",
      "100%|████████████████████████████████████████| 231M/231M [00:02<00:00, 87.2MB/s]\r\n",
      "/opt/conda/lib/python3.7/site-packages/gdown/cli.py:130: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\r\n",
      "  category=FutureWarning,\r\n",
      "Downloading...\r\n",
      "From: https://drive.google.com/uc?id=16Tiw2mAtHvdh3OZhffq-heG_j8d5ADZK\r\n",
      "To: /kaggle/working/vocab_file.txt\r\n",
      "100%|███████████████████████████████████████| 14.6M/14.6M [00:00<00:00, 135MB/s]\r\n",
      "/opt/conda/lib/python3.7/site-packages/gdown/cli.py:130: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\r\n",
      "  category=FutureWarning,\r\n",
      "Downloading...\r\n",
      "From: https://drive.google.com/uc?id=1Tlfc3caP6wAf7e_9URZJ5A2Mvt7Uc1U9\r\n",
      "To: /kaggle/working/AHS_model_arabic_extreme.model\r\n",
      "100%|███████████████████████████████████████| 6.56M/6.56M [00:00<00:00, 126MB/s]\r\n",
      "/opt/conda/lib/python3.7/site-packages/gdown/cli.py:130: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\r\n",
      "  category=FutureWarning,\r\n",
      "Downloading...\r\n",
      "From (uriginal): https://drive.google.com/uc?id=1uqJ_YkvBKjCUcKeawiOkvfgGPAydUWXe\r\n",
      "From (redirected): https://drive.google.com/uc?id=1uqJ_YkvBKjCUcKeawiOkvfgGPAydUWXe&confirm=t&uuid=d4fc0207-296e-45bb-a916-9ed059173d5f\r\n",
      "To: /kaggle/working/AHS_model_arabic_extreme.model.vectors.npy\r\n",
      "100%|████████████████████████████████████████| 108M/108M [00:01<00:00, 91.8MB/s]\r\n"
     ]
    }
   ],
   "source": [
    "! pip install gdown\n",
    "\n",
    "#AHS_clear Dataset\n",
    "!gdown --id 1ejVLlApcdG77h-zIL7NPbnaiPDsUooLV\n",
    "!gdown --id 1im9nJFcwz--jODqsKYD5qNqbr7st0UhC\n",
    "#AHS Dataset\n",
    "!gdown --id 122xV9jV2FCb9-cNiQPHG7dLoCkk2IsfF \n",
    "#download vocab file\n",
    "!gdown --id 16Tiw2mAtHvdh3OZhffq-heG_j8d5ADZK\n",
    "#download word embadding model\n",
    "!gdown --id 1Tlfc3caP6wAf7e_9URZJ5A2Mvt7Uc1U9\n",
    "!gdown --id 1uqJ_YkvBKjCUcKeawiOkvfgGPAydUWXe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f4f33e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:46:22.344173Z",
     "iopub.status.busy": "2023-04-16T21:46:22.343721Z",
     "iopub.status.idle": "2023-04-16T21:46:22.358527Z",
     "shell.execute_reply": "2023-04-16T21:46:22.357359Z"
    },
    "papermill": {
     "duration": 0.035525,
     "end_time": "2023-04-16T21:46:22.363442",
     "exception": false,
     "start_time": "2023-04-16T21:46:22.327917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# To Restore Model\\nimport gdown\\nurl='https://drive.google.com/drive/folders/1cEbqKr68i3x4-llV-oE4U5m-FHoncmvf'\\ngdown.download_folder(url, quiet=True)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# To Restore Model\n",
    "import gdown\n",
    "url='https://drive.google.com/drive/folders/1cEbqKr68i3x4-llV-oE4U5m-FHoncmvf'\n",
    "gdown.download_folder(url, quiet=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd384e3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:46:22.404081Z",
     "iopub.status.busy": "2023-04-16T21:46:22.403136Z",
     "iopub.status.idle": "2023-04-16T21:46:33.807908Z",
     "shell.execute_reply": "2023-04-16T21:46:33.806531Z"
    },
    "papermill": {
     "duration": 11.426605,
     "end_time": "2023-04-16T21:46:33.810730",
     "exception": false,
     "start_time": "2023-04-16T21:46:22.384125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arabic-reshaper\r\n",
      "  Downloading arabic_reshaper-3.0.0-py3-none-any.whl (20 kB)\r\n",
      "Installing collected packages: arabic-reshaper\r\n",
      "Successfully installed arabic-reshaper-3.0.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install arabic-reshaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3537dcb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:46:33.837716Z",
     "iopub.status.busy": "2023-04-16T21:46:33.836005Z",
     "iopub.status.idle": "2023-04-16T21:46:43.421827Z",
     "shell.execute_reply": "2023-04-16T21:46:43.420720Z"
    },
    "papermill": {
     "duration": 9.60209,
     "end_time": "2023-04-16T21:46:43.424842",
     "exception": false,
     "start_time": "2023-04-16T21:46:33.822752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "# Import TensorFlow >= 1.10 and enable eager execution\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from bidi.algorithm import get_display\n",
    "import arabic_reshaper\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import math \n",
    "\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.ticker as ticker\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba66a03c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:46:43.448188Z",
     "iopub.status.busy": "2023-04-16T21:46:43.447565Z",
     "iopub.status.idle": "2023-04-16T21:46:43.452396Z",
     "shell.execute_reply": "2023-04-16T21:46:43.451381Z"
    },
    "papermill": {
     "duration": 0.018721,
     "end_time": "2023-04-16T21:46:43.454490",
     "exception": false,
     "start_time": "2023-04-16T21:46:43.435769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path for the dataset file\n",
    "path_to_file =r'/kaggle/working/AHS_pandas.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d9e9685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:46:43.477483Z",
     "iopub.status.busy": "2023-04-16T21:46:43.477200Z",
     "iopub.status.idle": "2023-04-16T21:46:43.492796Z",
     "shell.execute_reply": "2023-04-16T21:46:43.491757Z"
    },
    "papermill": {
     "duration": 0.029697,
     "end_time": "2023-04-16T21:46:43.494993",
     "exception": false,
     "start_time": "2023-04-16T21:46:43.465296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ATSDataset:\n",
    "    def __init__(self, problem_type='ATS'):\n",
    "        self.problem_type = 'ATS'\n",
    "        self.inp_lang_tokenizer = None\n",
    "        self.targ_lang_tokenizer = None\n",
    "\n",
    "\n",
    "    def unicode_to_ascii(self, s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    ## Step 1 and Step 2 \n",
    "    def preprocess_sentence(self, w):\n",
    "        '''\n",
    "    Args:\n",
    "        w : A single word\n",
    "    Returns:\n",
    "        w : Single normalize word \n",
    "    \n",
    "    Convert Unicode to ASCII\n",
    "    Creating a space between a word and the punctuation following it\n",
    "    eg: \"he is a boy.\" => \"he is a boy .\" \n",
    "    Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    \n",
    "    Replacing everything with space except (a-z, A-Z, ا-ي \".\", \"?\", \"!\", \",\")\n",
    "    \n",
    "    Adding a start and an end token to the sentence\n",
    "    \n",
    "    '''\n",
    "        #w = self.unicode_to_ascii(w.strip())\n",
    "    \n",
    "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "        w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "        w = re.sub(r\"[^a-zA-Z؀-ۿ?.!,¿]+\", \" \", w)\n",
    "        w = w.rstrip().strip()\n",
    "    \n",
    "\n",
    "        # adding a start and an end token to the sentence\n",
    "        # so that the model know when to start and stop predicting.\n",
    "        w = '<start> %s <end>'%w\n",
    "        return w\n",
    "\n",
    "    def create_dataset(self, path, num_examples):\n",
    "        '''\n",
    "        Args:\n",
    "        path: Path of the dataset file\n",
    "        num_examples: Threshold to read a range from dataset\n",
    "        Returns:\n",
    "        word_pairs : All readed words from dataset as a pairs \n",
    "        \n",
    "        1. Remove the accents\n",
    "        2. Clean the sentences\n",
    "        3. Return word pairs in the format: [Article, Summary]\n",
    "        '''\n",
    "        lines = open(path, encoding='utf-8-sig').read().strip().split('\\n')\n",
    "        word_pairs = [[self.preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "        print(len(lines))\n",
    "        print(len(lines[:num_examples]))\n",
    "        return word_pairs\n",
    "\n",
    "    # Step 3 and Step 4\n",
    "    def tokenize(self, lang):\n",
    "        # lang = list of sentences in a language\n",
    "\n",
    "        # print(len(lang), \"example sentence: {}\".format(lang[0]))\n",
    "        lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n",
    "        lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "        ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n",
    "        ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n",
    "        tensor = lang_tokenizer.texts_to_sequences(lang) \n",
    "\n",
    "        ## tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
    "        ## and pads the sequences to match the longest sequences in the given input\n",
    "        tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "        return tensor, lang_tokenizer\n",
    "\n",
    "    def load_dataset(self, path, num_examples=None):\n",
    "        # creating cleaned input, output pairs\n",
    "        inp_lang,targ_lang = zip(*self.create_dataset(path, num_examples))\n",
    "\n",
    "        input_tensor, inp_lang_tokenizer = self.tokenize(inp_lang)\n",
    "        target_tensor, targ_lang_tokenizer = self.tokenize(targ_lang)\n",
    "\n",
    "        return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
    "\n",
    "    def call(self, num_examples, BUFFER_SIZE, BATCH_SIZE):\n",
    "        file_path = path_to_file\n",
    "        input_tensor, target_tensor, self.inp_lang_tokenizer, self.targ_lang_tokenizer = self.load_dataset(file_path, num_examples)\n",
    "\n",
    "        input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n",
    "        train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
    "        val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "        return train_dataset, val_dataset, self.inp_lang_tokenizer, self.targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afccd91d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:46:43.517177Z",
     "iopub.status.busy": "2023-04-16T21:46:43.516903Z",
     "iopub.status.idle": "2023-04-16T21:47:12.373654Z",
     "shell.execute_reply": "2023-04-16T21:47:12.372515Z"
    },
    "papermill": {
     "duration": 28.870903,
     "end_time": "2023-04-16T21:47:12.376391",
     "exception": false,
     "start_time": "2023-04-16T21:46:43.505488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589672\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 32000\n",
    "BATCH_SIZE = 64\n",
    "# Let's limit the #training examples for faster training\n",
    "num_examples = 100000\n",
    "\n",
    "dataset_creator = ATSDataset('ATS')\n",
    "train_dataset, val_dataset, inp_lang, targ_lang = dataset_creator.call(num_examples, BUFFER_SIZE, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e367fa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:47:12.402431Z",
     "iopub.status.busy": "2023-04-16T21:47:12.400535Z",
     "iopub.status.idle": "2023-04-16T21:47:12.615686Z",
     "shell.execute_reply": "2023-04-16T21:47:12.614647Z"
    },
    "papermill": {
     "duration": 0.22988,
     "end_time": "2023-04-16T21:47:12.618107",
     "exception": false,
     "start_time": "2023-04-16T21:47:12.388227",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 284]), TensorShape([64, 16]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(train_dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6691bc86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:47:12.642229Z",
     "iopub.status.busy": "2023-04-16T21:47:12.641264Z",
     "iopub.status.idle": "2023-04-16T21:47:12.647442Z",
     "shell.execute_reply": "2023-04-16T21:47:12.646383Z"
    },
    "papermill": {
     "duration": 0.020454,
     "end_time": "2023-04-16T21:47:12.649731",
     "exception": false,
     "start_time": "2023-04-16T21:47:12.629277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "max_length_input = example_input_batch.shape[1]\n",
    "max_length_output = example_target_batch.shape[1]\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "steps_per_epoch = num_examples//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bbee8e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:47:12.672340Z",
     "iopub.status.busy": "2023-04-16T21:47:12.672025Z",
     "iopub.status.idle": "2023-04-16T21:47:12.679597Z",
     "shell.execute_reply": "2023-04-16T21:47:12.678557Z"
    },
    "papermill": {
     "duration": 0.021256,
     "end_time": "2023-04-16T21:47:12.681651",
     "exception": false,
     "start_time": "2023-04-16T21:47:12.660395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_length_input, max_length_output, vocab_size_input, vocab_size_output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(284, 16, 179582, 26567)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"max_length_input, max_length_output, vocab_size_input, vocab_size_output\")\n",
    "max_length_input, max_length_output, vocab_inp_size, vocab_tar_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2c67445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:47:12.705879Z",
     "iopub.status.busy": "2023-04-16T21:47:12.705047Z",
     "iopub.status.idle": "2023-04-16T21:47:12.713738Z",
     "shell.execute_reply": "2023-04-16T21:47:12.712533Z"
    },
    "papermill": {
     "duration": 0.023342,
     "end_time": "2023-04-16T21:47:12.715983",
     "exception": false,
     "start_time": "2023-04-16T21:47:12.692641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### \n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    ##-------- LSTM layer in Encoder ------- ##\n",
    "        self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, h, c = self.lstm_layer(x, initial_state = hidden)\n",
    "        return output, h, c\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf5b6cf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:47:12.739824Z",
     "iopub.status.busy": "2023-04-16T21:47:12.739559Z",
     "iopub.status.idle": "2023-04-16T21:47:14.936100Z",
     "shell.execute_reply": "2023-04-16T21:47:14.933778Z"
    },
    "papermill": {
     "duration": 2.211316,
     "end_time": "2023-04-16T21:47:14.938810",
     "exception": false,
     "start_time": "2023-04-16T21:47:12.727494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 284, 1024)\n",
      "Encoder h vecotr shape: (batch size, units) (64, 1024)\n",
      "Encoder c vector shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Test Encoder Stack\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\n",
    "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b32472d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:47:14.965470Z",
     "iopub.status.busy": "2023-04-16T21:47:14.965138Z",
     "iopub.status.idle": "2023-04-16T21:47:14.977682Z",
     "shell.execute_reply": "2023-04-16T21:47:14.976598Z"
    },
    "papermill": {
     "duration": 0.028735,
     "end_time": "2023-04-16T21:47:14.980603",
     "exception": false,
     "start_time": "2023-04-16T21:47:14.951868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_type='luong'):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.attention_type = attention_type\n",
    "\n",
    "    # Embedding Layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    #Final Dense layer on which softmax will be applied\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # Define the fundamental cell for decoder recurrent structure\n",
    "        self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
    "    \n",
    "    # Sampler\n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "    # Create attention mechanism with memory = None\n",
    "        self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n",
    "                                                              None, self.batch_sz*[max_length_input], self.attention_type)\n",
    "\n",
    "    # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
    "        self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
    "\n",
    "    # Define the decoder with respect to fundamental rnn cell\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
    "\n",
    "\n",
    "    def build_rnn_cell(self, batch_sz):\n",
    "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n",
    "                                  self.attention_mechanism, attention_layer_size=self.dec_units)\n",
    "        return rnn_cell\n",
    "\n",
    "    def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n",
    "    # ------------- #\n",
    "    # typ: Which sort of attention (Bahdanau, Luong)\n",
    "    # dec_units: final dimension of attention outputs \n",
    "    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n",
    "    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
    "\n",
    "        if(attention_type=='bahdanau'):\n",
    "            return tfa.seq2seq.BahdanauAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "        else:\n",
    "            return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
    "\n",
    "    def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
    "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
    "        return decoder_initial_state\n",
    "\n",
    "\n",
    "    def call(self, inputs, initial_state):\n",
    "        x = self.embedding(inputs)\n",
    "        outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_output-1])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4c4839d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:47:15.005106Z",
     "iopub.status.busy": "2023-04-16T21:47:15.004144Z",
     "iopub.status.idle": "2023-04-16T21:47:18.959155Z",
     "shell.execute_reply": "2023-04-16T21:47:18.957162Z"
    },
    "papermill": {
     "duration": 3.969388,
     "end_time": "2023-04-16T21:47:18.961605",
     "exception": false,
     "start_time": "2023-04-16T21:47:14.992217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Outputs Shape:  (64, 15, 26567)\n"
     ]
    }
   ],
   "source": [
    "# Test decoder stack\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, 'luong')\n",
    "sample_x = tf.random.uniform((BATCH_SIZE, max_length_output))\n",
    "decoder.attention_mechanism.setup_memory(sample_output)\n",
    "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\n",
    "\n",
    "\n",
    "sample_decoder_outputs = decoder(sample_x, initial_state)\n",
    "\n",
    "print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "641fd653",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:47:18.986025Z",
     "iopub.status.busy": "2023-04-16T21:47:18.985165Z",
     "iopub.status.idle": "2023-04-16T21:47:18.995622Z",
     "shell.execute_reply": "2023-04-16T21:47:18.994655Z"
    },
    "papermill": {
     "duration": 0.024945,
     "end_time": "2023-04-16T21:47:18.997772",
     "exception": false,
     "start_time": "2023-04-16T21:47:18.972827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  # real shape = (BATCH_SIZE, max_length_output)\n",
    "  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
    "    cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = cross_entropy(y_true=real, y_pred=pred)\n",
    "    mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)  \n",
    "    loss = mask* loss\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95ca016e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:47:19.021427Z",
     "iopub.status.busy": "2023-04-16T21:47:19.021112Z",
     "iopub.status.idle": "2023-04-16T21:47:19.026287Z",
     "shell.execute_reply": "2023-04-16T21:47:19.025201Z"
    },
    "papermill": {
     "duration": 0.019553,
     "end_time": "2023-04-16T21:47:19.028731",
     "exception": false,
     "start_time": "2023-04-16T21:47:19.009178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db78254f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:47:19.052732Z",
     "iopub.status.busy": "2023-04-16T21:47:19.051919Z",
     "iopub.status.idle": "2023-04-16T21:47:19.059443Z",
     "shell.execute_reply": "2023-04-16T21:47:19.058484Z"
    },
    "papermill": {
     "duration": 0.021635,
     "end_time": "2023-04-16T21:47:19.061588",
     "exception": false,
     "start_time": "2023-04-16T21:47:19.039953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
    "        real = targ[ : , 1: ]         # ignore <start> token\n",
    "\n",
    "    # Set the AttentionMechanism object with encoder_outputs\n",
    "        decoder.attention_mechanism.setup_memory(enc_output)\n",
    "\n",
    "    # Create AttentionWrapperState as initial_state for decoder\n",
    "        decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
    "        pred = decoder(dec_input, decoder_initial_state)\n",
    "        logits = pred.rnn_output\n",
    "        loss = loss_function(real, logits)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36ffe203",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T21:47:19.086066Z",
     "iopub.status.busy": "2023-04-16T21:47:19.085785Z",
     "iopub.status.idle": "2023-04-16T23:27:21.398366Z",
     "shell.execute_reply": "2023-04-16T23:27:21.396902Z"
    },
    "papermill": {
     "duration": 6002.329407,
     "end_time": "2023-04-16T23:27:21.402523",
     "exception": false,
     "start_time": "2023-04-16T21:47:19.073116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.8973\n",
      "Epoch 1 Batch 100 Loss 1.8488\n",
      "Epoch 1 Batch 200 Loss 1.9416\n",
      "Epoch 1 Batch 300 Loss 1.7970\n",
      "Epoch 1 Batch 400 Loss 1.8095\n",
      "Epoch 1 Batch 500 Loss 1.6768\n",
      "Epoch 1 Batch 600 Loss 1.6252\n",
      "Epoch 1 Batch 700 Loss 1.5758\n",
      "Epoch 1 Batch 800 Loss 1.8248\n",
      "Epoch 1 Batch 900 Loss 1.5714\n",
      "Epoch 1 Batch 1000 Loss 1.7086\n",
      "Epoch 1 Batch 1100 Loss 1.4937\n",
      "Epoch 1 Batch 1200 Loss 1.6901\n",
      "Epoch 1 Loss 1.3803\n",
      "Time taken for 1 epoch 609.6581337451935 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.4772\n",
      "Epoch 2 Batch 100 Loss 1.4819\n",
      "Epoch 2 Batch 200 Loss 1.4771\n",
      "Epoch 2 Batch 300 Loss 1.3902\n",
      "Epoch 2 Batch 400 Loss 1.5351\n",
      "Epoch 2 Batch 500 Loss 1.2664\n",
      "Epoch 2 Batch 600 Loss 1.2992\n",
      "Epoch 2 Batch 700 Loss 1.3993\n",
      "Epoch 2 Batch 800 Loss 1.4016\n",
      "Epoch 2 Batch 900 Loss 1.2949\n",
      "Epoch 2 Batch 1000 Loss 1.3751\n",
      "Epoch 2 Batch 1100 Loss 1.4495\n",
      "Epoch 2 Batch 1200 Loss 1.3433\n",
      "Epoch 2 Loss 1.1423\n",
      "Time taken for 1 epoch 600.9999649524689 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.2010\n",
      "Epoch 3 Batch 100 Loss 1.2918\n",
      "Epoch 3 Batch 200 Loss 1.3537\n",
      "Epoch 3 Batch 300 Loss 1.3021\n",
      "Epoch 3 Batch 400 Loss 1.2556\n",
      "Epoch 3 Batch 500 Loss 1.1936\n",
      "Epoch 3 Batch 600 Loss 1.2299\n",
      "Epoch 3 Batch 700 Loss 1.1331\n",
      "Epoch 3 Batch 800 Loss 1.1922\n",
      "Epoch 3 Batch 900 Loss 1.2972\n",
      "Epoch 3 Batch 1000 Loss 1.2971\n",
      "Epoch 3 Batch 1100 Loss 1.2848\n",
      "Epoch 3 Batch 1200 Loss 1.2595\n",
      "Epoch 3 Loss 1.0178\n",
      "Time taken for 1 epoch 598.7723815441132 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.1240\n",
      "Epoch 4 Batch 100 Loss 1.1402\n",
      "Epoch 4 Batch 200 Loss 1.1913\n",
      "Epoch 4 Batch 300 Loss 1.1615\n",
      "Epoch 4 Batch 400 Loss 1.1406\n",
      "Epoch 4 Batch 500 Loss 1.1003\n",
      "Epoch 4 Batch 600 Loss 1.2387\n",
      "Epoch 4 Batch 700 Loss 1.2444\n",
      "Epoch 4 Batch 800 Loss 1.1007\n",
      "Epoch 4 Batch 900 Loss 1.2473\n",
      "Epoch 4 Batch 1000 Loss 1.1581\n",
      "Epoch 4 Batch 1100 Loss 1.1619\n",
      "Epoch 4 Batch 1200 Loss 1.2070\n",
      "Epoch 4 Loss 0.9220\n",
      "Time taken for 1 epoch 600.9355475902557 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.0267\n",
      "Epoch 5 Batch 100 Loss 0.9676\n",
      "Epoch 5 Batch 200 Loss 0.9250\n",
      "Epoch 5 Batch 300 Loss 1.0346\n",
      "Epoch 5 Batch 400 Loss 0.9679\n",
      "Epoch 5 Batch 500 Loss 1.0844\n",
      "Epoch 5 Batch 600 Loss 1.1199\n",
      "Epoch 5 Batch 700 Loss 1.1023\n",
      "Epoch 5 Batch 800 Loss 0.9965\n",
      "Epoch 5 Batch 900 Loss 1.0518\n",
      "Epoch 5 Batch 1000 Loss 1.0800\n",
      "Epoch 5 Batch 1100 Loss 1.0197\n",
      "Epoch 5 Batch 1200 Loss 1.0204\n",
      "Epoch 5 Loss 0.8435\n",
      "Time taken for 1 epoch 597.6732528209686 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.9866\n",
      "Epoch 6 Batch 100 Loss 0.9372\n",
      "Epoch 6 Batch 200 Loss 0.9404\n",
      "Epoch 6 Batch 300 Loss 0.8946\n",
      "Epoch 6 Batch 400 Loss 0.9609\n",
      "Epoch 6 Batch 500 Loss 0.9526\n",
      "Epoch 6 Batch 600 Loss 0.9584\n",
      "Epoch 6 Batch 700 Loss 0.9319\n",
      "Epoch 6 Batch 800 Loss 1.0289\n",
      "Epoch 6 Batch 900 Loss 1.0654\n",
      "Epoch 6 Batch 1000 Loss 0.9241\n",
      "Epoch 6 Batch 1100 Loss 0.9271\n",
      "Epoch 6 Batch 1200 Loss 1.0068\n",
      "Epoch 6 Loss 0.7680\n",
      "Time taken for 1 epoch 599.3531601428986 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.8644\n",
      "Epoch 7 Batch 100 Loss 0.8509\n",
      "Epoch 7 Batch 200 Loss 0.9108\n",
      "Epoch 7 Batch 300 Loss 0.8832\n",
      "Epoch 7 Batch 400 Loss 0.8739\n",
      "Epoch 7 Batch 500 Loss 0.9212\n",
      "Epoch 7 Batch 600 Loss 0.9082\n",
      "Epoch 7 Batch 700 Loss 0.9337\n",
      "Epoch 7 Batch 800 Loss 0.9300\n",
      "Epoch 7 Batch 900 Loss 0.8813\n",
      "Epoch 7 Batch 1000 Loss 0.8859\n",
      "Epoch 7 Batch 1100 Loss 0.9209\n",
      "Epoch 7 Batch 1200 Loss 0.8883\n",
      "Epoch 7 Loss 0.7094\n",
      "Time taken for 1 epoch 597.1386482715607 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.7484\n",
      "Epoch 8 Batch 100 Loss 0.7580\n",
      "Epoch 8 Batch 200 Loss 0.7878\n",
      "Epoch 8 Batch 300 Loss 0.7747\n",
      "Epoch 8 Batch 400 Loss 0.8608\n",
      "Epoch 8 Batch 500 Loss 0.7648\n",
      "Epoch 8 Batch 600 Loss 0.8092\n",
      "Epoch 8 Batch 700 Loss 0.9323\n",
      "Epoch 8 Batch 800 Loss 0.8532\n",
      "Epoch 8 Batch 900 Loss 0.8195\n",
      "Epoch 8 Batch 1000 Loss 0.7795\n",
      "Epoch 8 Batch 1100 Loss 0.8414\n",
      "Epoch 8 Batch 1200 Loss 0.7916\n",
      "Epoch 8 Loss 0.6563\n",
      "Time taken for 1 epoch 599.9786167144775 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.7325\n",
      "Epoch 9 Batch 100 Loss 0.6979\n",
      "Epoch 9 Batch 200 Loss 0.9184\n",
      "Epoch 9 Batch 300 Loss 0.8504\n",
      "Epoch 9 Batch 400 Loss 0.6946\n",
      "Epoch 9 Batch 500 Loss 0.7244\n",
      "Epoch 9 Batch 600 Loss 0.7827\n",
      "Epoch 9 Batch 700 Loss 0.7496\n",
      "Epoch 9 Batch 800 Loss 0.7916\n",
      "Epoch 9 Batch 900 Loss 0.7705\n",
      "Epoch 9 Batch 1000 Loss 0.6934\n",
      "Epoch 9 Batch 1100 Loss 0.7117\n",
      "Epoch 9 Batch 1200 Loss 0.7451\n",
      "Epoch 9 Loss 0.6112\n",
      "Time taken for 1 epoch 597.6578092575073 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.6212\n",
      "Epoch 10 Batch 100 Loss 0.6223\n",
      "Epoch 10 Batch 200 Loss 0.6004\n",
      "Epoch 10 Batch 300 Loss 0.6916\n",
      "Epoch 10 Batch 400 Loss 0.6814\n",
      "Epoch 10 Batch 500 Loss 0.7733\n",
      "Epoch 10 Batch 600 Loss 0.6953\n",
      "Epoch 10 Batch 700 Loss 0.5913\n",
      "Epoch 10 Batch 800 Loss 0.6258\n",
      "Epoch 10 Batch 900 Loss 0.6323\n",
      "Epoch 10 Batch 1000 Loss 0.6870\n",
      "Epoch 10 Batch 1100 Loss 0.7118\n",
      "Epoch 10 Batch 1200 Loss 0.6692\n",
      "Epoch 10 Loss 0.5373\n",
      "Time taken for 1 epoch 600.1283867359161 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "  # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3edca985",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T23:27:21.543974Z",
     "iopub.status.busy": "2023-04-16T23:27:21.543353Z",
     "iopub.status.idle": "2023-04-16T23:27:21.571198Z",
     "shell.execute_reply": "2023-04-16T23:27:21.569394Z"
    },
    "papermill": {
     "duration": 0.094552,
     "end_time": "2023-04-16T23:27:21.575221",
     "exception": false,
     "start_time": "2023-04-16T23:27:21.480669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_sentence(sentence):\n",
    "    sentence = dataset_creator.preprocess_sentence(sentence)\n",
    "    \n",
    "    ##This is a proplem##############\n",
    "    inputs=''\n",
    "    try:\n",
    "        inputs = [inp_lang.word_index[i] for i in sentence.split()]\n",
    "    except:\n",
    "        pass\n",
    "    ############################\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen=max_length_input,\n",
    "                                                          padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    result = ''\n",
    "\n",
    "    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "\n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "    \n",
    "    start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n",
    "    end_token = targ_lang.word_index['<end>']\n",
    "\n",
    "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "    \n",
    "  # Instantiate BasicDecoder object\n",
    "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
    "  # Setup Memory in decoder stack\n",
    "    decoder.attention_mechanism.setup_memory(enc_out)\n",
    "\n",
    "  # set decoder_initial_state\n",
    "    decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
    "    \n",
    "\n",
    "  ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
    "  ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
    "  ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
    "\n",
    "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n",
    "    \n",
    "    return outputs.sample_id.numpy()\n",
    "\n",
    "def Summary(sentence):\n",
    "    result = evaluate_sentence(sentence)\n",
    "    #print(result)\n",
    "    result = targ_lang.sequences_to_texts(result)\n",
    "    return result\n",
    "    #print('Input: %s' % (sentence))\n",
    "    #print('Predicted summarization: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb720fce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T23:27:21.702164Z",
     "iopub.status.busy": "2023-04-16T23:27:21.701572Z",
     "iopub.status.idle": "2023-04-16T23:27:22.973658Z",
     "shell.execute_reply": "2023-04-16T23:27:22.972661Z"
    },
    "papermill": {
     "duration": 1.337083,
     "end_time": "2023-04-16T23:27:22.976235",
     "exception": false,
     "start_time": "2023-04-16T23:27:21.639152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7a325dcca490>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de190f7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T23:27:23.015023Z",
     "iopub.status.busy": "2023-04-16T23:27:23.014653Z",
     "iopub.status.idle": "2023-04-16T23:27:26.406847Z",
     "shell.execute_reply": "2023-04-16T23:27:26.405635Z"
    },
    "papermill": {
     "duration": 3.414044,
     "end_time": "2023-04-16T23:27:26.409496",
     "exception": false,
     "start_time": "2023-04-16T23:27:22.995452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/kaggle/working/AHS_clear.csv')[:num_examples]\n",
    "df = df.filter(['Text', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a33a30d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T23:27:26.448269Z",
     "iopub.status.busy": "2023-04-16T23:27:26.447959Z",
     "iopub.status.idle": "2023-04-16T23:27:26.551138Z",
     "shell.execute_reply": "2023-04-16T23:27:26.550192Z"
    },
    "papermill": {
     "duration": 0.124313,
     "end_time": "2023-04-16T23:27:26.553234",
     "exception": false,
     "start_time": "2023-04-16T23:27:26.428921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['مدينه الناظور <end>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Summary(df['Text'][90000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "423d5205",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T23:27:26.591874Z",
     "iopub.status.busy": "2023-04-16T23:27:26.591026Z",
     "iopub.status.idle": "2023-04-16T23:27:26.598284Z",
     "shell.execute_reply": "2023-04-16T23:27:26.597208Z"
    },
    "papermill": {
     "duration": 0.028844,
     "end_time": "2023-04-16T23:27:26.600861",
     "exception": false,
     "start_time": "2023-04-16T23:27:26.572017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'مدينه الضمير'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Summary'][90000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7d8a559",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-16T23:27:26.638456Z",
     "iopub.status.busy": "2023-04-16T23:27:26.638169Z",
     "iopub.status.idle": "2023-04-17T01:12:16.269138Z",
     "shell.execute_reply": "2023-04-17T01:12:16.268008Z"
    },
    "papermill": {
     "duration": 6289.652743,
     "end_time": "2023-04-17T01:12:16.271967",
     "exception": false,
     "start_time": "2023-04-16T23:27:26.619224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Text_list,Predicted_list,Summary_list=[],[],[]\n",
    "for text,summary in zip(df['Text'],df['Summary']):\n",
    "    Text_list.append(text)\n",
    "    Predicted_list.append(Summary(text))\n",
    "    Summary_list.append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "724ab6d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T01:12:16.312442Z",
     "iopub.status.busy": "2023-04-17T01:12:16.310682Z",
     "iopub.status.idle": "2023-04-17T01:12:17.293185Z",
     "shell.execute_reply": "2023-04-17T01:12:17.292133Z"
    },
    "papermill": {
     "duration": 1.004765,
     "end_time": "2023-04-17T01:12:17.295972",
     "exception": false,
     "start_time": "2023-04-17T01:12:16.291207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dic={'Text':Text_list,'Predicted Summary':Predicted_list,'Real Summary':Summary_list}\n",
    "ds=pd.DataFrame(dic)\n",
    "ds.to_csv('out.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca97319f",
   "metadata": {
    "papermill": {
     "duration": 0.018079,
     "end_time": "2023-04-17T01:12:17.333081",
     "exception": false,
     "start_time": "2023-04-17T01:12:17.315002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91fa6e99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T01:12:17.371141Z",
     "iopub.status.busy": "2023-04-17T01:12:17.370837Z",
     "iopub.status.idle": "2023-04-17T01:12:17.383374Z",
     "shell.execute_reply": "2023-04-17T01:12:17.382253Z"
    },
    "papermill": {
     "duration": 0.034327,
     "end_time": "2023-04-17T01:12:17.385499",
     "exception": false,
     "start_time": "2023-04-17T01:12:17.351172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beam_evaluate_sentence(sentence, beam_width=3):\n",
    "    sentence = dataset_creator.preprocess_sentence(sentence)\n",
    "\n",
    "    ##This is a proplem##############\n",
    "    inputs=''\n",
    "    try:\n",
    "        inputs = [inp_lang.word_index[i] for i in sentence.split()]\n",
    "    except:\n",
    "        pass\n",
    "    ############################\n",
    "    \n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                          maxlen=max_length_input,\n",
    "                                                          padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    result = ''\n",
    "\n",
    "    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
    "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
    "\n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "\n",
    "    start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n",
    "    end_token = targ_lang.word_index['<end>']\n",
    "\n",
    "  # From official documentation\n",
    "  # NOTE If you are using the BeamSearchDecoder with a cell wrapped in AttentionWrapper, then you must ensure that:\n",
    "  # The encoder output has been tiled to beam_width via tfa.seq2seq.tile_batch (NOT tf.tile).\n",
    "  # The batch_size argument passed to the get_initial_state method of this wrapper is equal to true_batch_size * beam_width.\n",
    "  # The initial state created with get_initial_state above contains a cell_state value containing properly tiled final state from the encoder.\n",
    "\n",
    "    enc_out = tfa.seq2seq.tile_batch(enc_out, multiplier=beam_width)\n",
    "    decoder.attention_mechanism.setup_memory(enc_out)\n",
    "    print(\"beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] :\", enc_out.shape)\n",
    "\n",
    "  # set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
    "    hidden_state = tfa.seq2seq.tile_batch([enc_h, enc_c], multiplier=beam_width)\n",
    "    decoder_initial_state = decoder.rnn_cell.get_initial_state(batch_size=beam_width*inference_batch_size, dtype=tf.float32)\n",
    "    decoder_initial_state = decoder_initial_state.clone(cell_state=hidden_state)\n",
    "\n",
    "  # Instantiate BeamSearchDecoder\n",
    "    decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoder.rnn_cell,beam_width=beam_width, output_layer=decoder.fc)\n",
    "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
    "\n",
    "  # The BeamSearchDecoder object's call() function takes care of everything.\n",
    "    outputs, final_state, sequence_lengths = decoder_instance(decoder_embedding_matrix, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\n",
    "  # outputs is tfa.seq2seq.FinalBeamSearchDecoderOutput object. \n",
    "  # The final beam predictions are stored in outputs.predicted_id\n",
    "  # outputs.beam_search_decoder_output is a tfa.seq2seq.BeamSearchDecoderOutput object which keep tracks of beam_scores and parent_ids while performing a beam decoding step\n",
    "  # final_state = tfa.seq2seq.BeamSearchDecoderState object.\n",
    "  # Sequence Length = [inference_batch_size, beam_width] details the maximum length of the beams that are generated\n",
    "\n",
    "\n",
    "  # outputs.predicted_id.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
    "  # outputs.beam_search_decoder_output.scores.shape = (inference_batch_size, time_step_outputs, beam_width)\n",
    "  # Convert the shape of outputs and beam_scores to (inference_batch_size, beam_width, time_step_outputs)\n",
    "    final_outputs = tf.transpose(outputs.predicted_ids, perm=(0,2,1))\n",
    "    beam_scores = tf.transpose(outputs.beam_search_decoder_output.scores, perm=(0,2,1))\n",
    "\n",
    "    return final_outputs.numpy(), beam_scores.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb1fc95e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T01:12:17.423649Z",
     "iopub.status.busy": "2023-04-17T01:12:17.422855Z",
     "iopub.status.idle": "2023-04-17T01:12:17.429894Z",
     "shell.execute_reply": "2023-04-17T01:12:17.428917Z"
    },
    "papermill": {
     "duration": 0.028262,
     "end_time": "2023-04-17T01:12:17.431911",
     "exception": false,
     "start_time": "2023-04-17T01:12:17.403649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beam_summary(sentence):\n",
    "    result, beam_scores = beam_evaluate_sentence(sentence)\n",
    "    print(result.shape, beam_scores.shape)\n",
    "    for beam, score in zip(result, beam_scores):\n",
    "        print(beam.shape, score.shape)\n",
    "        output = targ_lang.sequences_to_texts(beam)\n",
    "        output = [a[:a.index('<end>')] for a in output]\n",
    "        beam_score = [a.sum() for a in score]\n",
    "        print('Input: %s' % (sentence))\n",
    "        for i in range(len(output)):\n",
    "            print('{} Predicted summarization: {}  {}'.format(i+1, beam_score[i], output[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6547c351",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-17T01:12:17.469412Z",
     "iopub.status.busy": "2023-04-17T01:12:17.469120Z",
     "iopub.status.idle": "2023-04-17T01:12:18.941711Z",
     "shell.execute_reply": "2023-04-17T01:12:18.940630Z"
    },
    "papermill": {
     "duration": 1.494146,
     "end_time": "2023-04-17T01:12:18.944129",
     "exception": false,
     "start_time": "2023-04-17T01:12:17.449983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] : (3, 284, 1024)\n",
      "(1, 3, 5) (1, 3, 5)\n",
      "(3, 5) (3, 5)\n",
      "Input: تساعد الزيوت العطريه الاساسيه علاج تساقط الشعر  فقد تم استعمال زيت اللافندر بنجاح الاشخاص يعانون الصلع  يتم مزج الزيوت ببعضها  فمثلا يمكن مزج زيت زيت الزعتر زيت الروزماري  توجد ادله كافيه تثبت فعاليه الزيوت علاج تساقط الشعر  وينصح استعمالها بوضع قطرات الجلد الانتظار لمده  ساعه  للتاكد كانت تسبب الحساسيه \n",
      "1 Predicted summarization: -9.383207321166992  الزيوت المستخده في التدليك \n",
      "2 Predicted summarization: -14.324695587158203  الزيوت العطريه \n",
      "3 Predicted summarization: -15.911822319030762  استخدام الزيوت الطبيعيه \n",
      "None Real Summary:  الزيوت العطريه \n",
      "\n",
      "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] : (3, 284, 1024)\n",
      "(1, 3, 7) (1, 3, 7)\n",
      "(3, 7) (3, 7)\n",
      "Input: يجب التركيز تناول الاطعمه الغنيه بالفيتامينات المعادن المختلفه للحفاظ صحه الشعر  فالشعر الصحي يعتمد نوعيه الطعام المتناول  يحتاج الشعر الحديد  الزنك  حمض الفوليك \n",
      "1 Predicted summarization: -17.42173194885254  تناول نظام غذائي متوازن \n",
      "2 Predicted summarization: -19.321388244628906  الاكثار من تناول البروتينات \n",
      "3 Predicted summarization: -23.702289581298828  تناول الاطعمه الغنيه بالبوتاسيوم و المغنيسيوم \n",
      "None Real Summary:  تناول الاطعمه الغنيه بالمعادن و الفيتامينات \n",
      "\n",
      "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] : (3, 284, 1024)\n",
      "(1, 3, 4) (1, 3, 4)\n",
      "(3, 4) (3, 4)\n",
      "Input: يمكن تعريف هبوط ضغط الدم  بالانجليزيه  Hypotension  بانه انخفاض ضغط الدم الانقباضي  بالانجليزيه  Systolic Blood Pressure  ضغط الشراين الناتج تدفق الدم خلال خفقان القلب   ملم زئبقي  انخفاض ضغط الدم الانبساطي  بالانجليزيه  Diastolic Blood Pressure  ضغط الشراين خلال انبساط القلب   ملم زئبقي  عاده يكون انخفاض الدم امرا مرغوبا يستدعي القلق  يصاحبه ظهور اعراض معينه  انه الحالات يكون علامه وجود مشكله صحيه  يؤدي نقصان تدفق الدم القلب  الدماغ  اعضاء اخري الجسم  تجدر الاشاره القراءه الطبيعيه لضغط الدم عندما يكون الضغط الانقباضي  ملم زئبقي الضغط الانبساطي  ملم زئبقي \n",
      "1 Predicted summarization: -16.67938232421875  نتائج الثوره الفرنسيه \n",
      "2 Predicted summarization: -20.84941291809082  نتائج الثوره الصناعيه \n",
      "3 Predicted summarization: -22.13865852355957  تاريخ الزباره \n",
      "None Real Summary:  هبوط ضغط الدم \n",
      "\n",
      "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] : (3, 284, 1024)\n",
      "(1, 3, 7) (1, 3, 7)\n",
      "(3, 7) (3, 7)\n",
      "Input: يتغير ضغط الدم خلال اليوم اعتمادا عده عوامل  وضعيه الجسم  التنفس  ودرجه التوتر النفسي  الوضع البدني  الاكل  الشرب  الادويه المستخدمه  بالاضافه تغير الوقت  ضغط الدم عاده يكون انخفاضا خلال الليل  يرتفع بحده الاستيقاظ النوم  اسباب هبوط ضغط الدم ياتي \n",
      "1 Predicted summarization: -12.895694732666016  ارتفاع ضغط الدم \n",
      "2 Predicted summarization: -16.136959075927734  ارتفاع ضغط الدم الانقباضي \n",
      "3 Predicted summarization: -25.908180236816406  ارتفاع ضغط الدم الانقباضي و الانبساطي \n",
      "None Real Summary:  اسباب هبوط ضغط الدم \n",
      "\n",
      "beam_with * [batch_size, max_length_input, rnn_units] :  3 * [1, 16, 1024]] : (3, 284, 1024)\n",
      "(1, 3, 4) (1, 3, 4)\n",
      "(3, 4) (3, 4)\n",
      "Input: العديد الاعراض تصاحب هبوط ضغط الدم  تجدر الاشاره الصدمه  بالانجليزيه  Shock  حاله طارئه مهدده لحياه المصاب  تنتج الانخفاض الشديد لضغط الدم تسبب الشعور بالتشويش  مشاكل التنفس  وضعف ضربات القلب زياده سرعتها  اعراض هبوط الضغط ياتي \n",
      "1 Predicted summarization: -16.67938232421875  نتائج الثوره الفرنسيه \n",
      "2 Predicted summarization: -20.84941291809082  نتائج الثوره الصناعيه \n",
      "3 Predicted summarization: -22.13865852355957  تاريخ الزباره \n",
      "None Real Summary:  اعراض هبوط ضغط الدم \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text,summary in zip(df['Text'][-5:],df['Summary'][-5:]):\n",
    "    print(beam_summary(text),'Real Summary: ',summary,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4eaf77",
   "metadata": {
    "papermill": {
     "duration": 0.018393,
     "end_time": "2023-04-17T01:12:18.981737",
     "exception": false,
     "start_time": "2023-04-17T01:12:18.963344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12402.950223,
   "end_time": "2023-04-17T01:12:22.150785",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-16T21:45:39.200562",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
